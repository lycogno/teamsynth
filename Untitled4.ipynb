{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UJLUaySU3YsI"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "iZcRKhMe8Pcg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers\n",
        "import transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOpCxdGe3eUN",
        "outputId": "b2851079-1dbe-4aba-8141-9190f9bba139"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "essays_df = pd.read_csv('essays.csv', encoding='latin1')\n",
        "cat_columns = ['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']\n",
        "essays_df[cat_columns] = essays_df[cat_columns].replace({'y': 1, 'n': 0})\n",
        "essays_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "wJwX02gj4VXs",
        "outputId": "795a84ec-0445-4a9e-eb4a-15d031b507d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              #AUTHID                                               TEXT  \\\n",
              "0     1997_504851.txt  Well, right now I just woke up from a mid-day ...   \n",
              "1     1997_605191.txt  Well, here we go with the stream of consciousn...   \n",
              "2     1997_687252.txt  An open keyboard and buttons to push. The thin...   \n",
              "3     1997_568848.txt  I can't believe it!  It's really happening!  M...   \n",
              "4     1997_688160.txt  Well, here I go with the good old stream of co...   \n",
              "...               ...                                                ...   \n",
              "2462     2004_493.txt       I'm home. wanted to go to bed but remembe...   \n",
              "2463     2004_494.txt       Stream of consiousnesssskdj. How do you s...   \n",
              "2464     2004_497.txt  It is Wednesday, December 8th and a lot has be...   \n",
              "2465     2004_498.txt  Man this week has been hellish. Anyways, now i...   \n",
              "2466     2004_499.txt  I have just gotten off the phone with brady. I...   \n",
              "\n",
              "      cEXT  cNEU  cAGR  cCON  cOPN  \n",
              "0        0     1     1     0     1  \n",
              "1        0     0     1     0     0  \n",
              "2        0     1     0     1     1  \n",
              "3        1     0     1     1     0  \n",
              "4        1     0     1     0     1  \n",
              "...    ...   ...   ...   ...   ...  \n",
              "2462     0     1     0     1     0  \n",
              "2463     1     1     0     0     1  \n",
              "2464     0     0     1     0     0  \n",
              "2465     0     1     0     0     1  \n",
              "2466     0     1     1     0     1  \n",
              "\n",
              "[2467 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c9ed8a01-57b0-4895-aa3f-58014f1d812d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>#AUTHID</th>\n",
              "      <th>TEXT</th>\n",
              "      <th>cEXT</th>\n",
              "      <th>cNEU</th>\n",
              "      <th>cAGR</th>\n",
              "      <th>cCON</th>\n",
              "      <th>cOPN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1997_504851.txt</td>\n",
              "      <td>Well, right now I just woke up from a mid-day ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1997_605191.txt</td>\n",
              "      <td>Well, here we go with the stream of consciousn...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1997_687252.txt</td>\n",
              "      <td>An open keyboard and buttons to push. The thin...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1997_568848.txt</td>\n",
              "      <td>I can't believe it!  It's really happening!  M...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1997_688160.txt</td>\n",
              "      <td>Well, here I go with the good old stream of co...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2462</th>\n",
              "      <td>2004_493.txt</td>\n",
              "      <td>I'm home. wanted to go to bed but remembe...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2463</th>\n",
              "      <td>2004_494.txt</td>\n",
              "      <td>Stream of consiousnesssskdj. How do you s...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2464</th>\n",
              "      <td>2004_497.txt</td>\n",
              "      <td>It is Wednesday, December 8th and a lot has be...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2465</th>\n",
              "      <td>2004_498.txt</td>\n",
              "      <td>Man this week has been hellish. Anyways, now i...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2466</th>\n",
              "      <td>2004_499.txt</td>\n",
              "      <td>I have just gotten off the phone with brady. I...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2467 rows Ã— 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9ed8a01-57b0-4895-aa3f-58014f1d812d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c9ed8a01-57b0-4895-aa3f-58014f1d812d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c9ed8a01-57b0-4895-aa3f-58014f1d812d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-55b04982-75b6-4f1b-8a75-fdbbe605fd3f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-55b04982-75b6-4f1b-8a75-fdbbe605fd3f')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-55b04982-75b6-4f1b-8a75-fdbbe605fd3f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mbti_df = pd.read_csv('mbti_1.csv', error_bad_lines=False)\n",
        "# types = mbti_df['type'].unique()\n",
        "# types = dict(zip(types, range(len(types))))\n",
        "# mbti_df['type'] = mbti_df['type'].replace(types)\n",
        "# mbti_df"
      ],
      "metadata": {
        "id": "MSlHPQCi4oxE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "config = json.load(open('config.json'))"
      ],
      "metadata": {
        "id": "H2-A2bOB6oiS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_columns = ['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']\n",
        "\n",
        "running_loss_essay = 0\n",
        "running_loss_mbti = 0"
      ],
      "metadata": {
        "id": "XpGZm83J5GK6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from transformers import DistilBertModel, BertTokenizer\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class BERTEncoder(nn.Module):\n",
        "    def __init__(self, config, is_training = True):\n",
        "        super(BERTEncoder, self).__init__()\n",
        "        self.enc =  DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.att = nn.Linear(768, 1)\n",
        "        self.fc = nn.Linear(768, config['embedding_size'])\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        src = self.enc(input_ids=tokens['input_ids'], attention_mask=tokens['attention_mask'], return_dict=True)\n",
        "        src = src.last_hidden_state\n",
        "\n",
        "        wt = self.att(src)\n",
        "        # wt = [batch size, src len, 1]\n",
        "        wt = torch.softmax(wt, dim=1)\n",
        "        src = torch.matmul(wt.permute(0, 2, 1), src)\n",
        "        # src = [batch size, 1, 768]\n",
        "        src = self.fc(src)\n",
        "        # src = [batch size, 1, embed_size]\n",
        "        src = torch.squeeze(src, dim=1)\n",
        "        # src = [batch size, embed_size]\n",
        "        return src\n",
        "\n",
        "\n",
        "class APP(nn.Module):\n",
        "\n",
        "    def __init__(self, config, is_training=True):\n",
        "        super(APP, self).__init__()\n",
        "        self._text_encoder = BERTEncoder(config, is_training)\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self._fc1 = nn.Linear(config['embedding_size'], config['hidden_size'])\n",
        "        self._fc2 = nn.Linear(config['hidden_size'], config['pers_embedding_size'])\n",
        "\n",
        "        self.mbti_classifier = nn.Linear(config['pers_embedding_size'], 16)\n",
        "\n",
        "        self.OCEAN_layer = nn.Linear(config['pers_embedding_size'], config['ocean_size'])\n",
        "\n",
        "        self.O_classifier = nn.Linear(config['ocean_size'], 2)\n",
        "        self.C_classifier = nn.Linear(config['ocean_size'], 2)\n",
        "        self.E_classifier = nn.Linear(config['ocean_size'], 2)\n",
        "        self.A_classifier = nn.Linear(config['ocean_size'], 2)\n",
        "        self.N_classifier = nn.Linear(config['ocean_size'], 2)\n",
        "\n",
        "    def get_ocean_loss(self, predictions, labels):\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        OCEAN_loss = 0\n",
        "        for cat in ['cOPN', 'cCON', 'cEXT', 'cAGR', 'cNEU']:\n",
        "            OCEAN_loss += criterion(predictions[cat], labels[cat].long().to(device))\n",
        "\n",
        "        return OCEAN_loss\n",
        "\n",
        "    def get_mbti_loss(self, predictions, labels):\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        mbti_loss = criterion(predictions['mbti'], torch.Tensor([labels]).long().to(device))\n",
        "        return mbti_loss\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        config = self.config\n",
        "        # get text embeddings\n",
        "        text_embeddings = self._text_encoder(tokens)\n",
        "\n",
        "        # get personality embeddings\n",
        "        personality_embeddings = self._fc2(F.relu(self._fc1(text_embeddings)))\n",
        "\n",
        "        # get mbti\n",
        "        mbti = self.mbti_classifier(F.relu(personality_embeddings))\n",
        "\n",
        "        # get OCEAN\n",
        "        OCEAN = F.relu(self.OCEAN_layer(F.relu(personality_embeddings)))\n",
        "        O_pred = self.O_classifier(OCEAN)\n",
        "        C_pred = self.C_classifier(OCEAN)\n",
        "        E_pred = self.E_classifier(OCEAN)\n",
        "        A_pred = self.A_classifier(OCEAN)\n",
        "        N_pred = self.N_classifier(OCEAN)\n",
        "\n",
        "        predictions = {\n",
        "            'mbti': mbti,\n",
        "            'cOPN': O_pred,\n",
        "            'cCON': C_pred,\n",
        "            'cEXT': E_pred,\n",
        "            'cAGR': A_pred,\n",
        "            'cNEU': N_pred,\n",
        "            'personality_embeddings': personality_embeddings,\n",
        "        }\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "TrVztpsB6uin"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "class Essays_dataset(Dataset):\n",
        "\n",
        "    def __init__(self, file_name):\n",
        "        df = pd.read_csv(file_name ,encoding='latin1')\n",
        "\n",
        "        self.tokeniser = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        cat_columns = ['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']\n",
        "        df[cat_columns] = df[cat_columns].replace({'y': 1, 'n': 0})\n",
        "\n",
        "        self.X = df.drop(cat_columns, axis=1)\n",
        "        self.y = df[cat_columns]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        text = self.X.iloc[idx]['TEXT']\n",
        "        tokens = self.tokeniser(text, add_special_tokens=True, padding='max_length', truncation=True, return_tensors='pt', max_length=512)\n",
        "        # tokens = {'input_ids': tokens['input_ids'], 'token_type_ids': tokens['token_type_ids']}\n",
        "        tokens['input_ids'] = tokens['input_ids'].squeeze(0).to(device)\n",
        "        tokens['attention_mask'] = tokens['attention_mask'].squeeze(0).to(device)\n",
        "        labels = self.y.iloc[idx].to_dict()\n",
        "\n",
        "        return tokens, labels"
      ],
      "metadata": {
        "id": "6Dl_cMNzqGNF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = Essays_dataset('essays.csv')\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "u6lBl-kJqHsw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dataloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAhoG0bwqNuX",
        "outputId": "1797d2e5-921c-4de6-97eb-19ce9ef94c76"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'input_ids': tensor([[ 101, 1045, 2633,  ...,    0,    0,    0],\n",
              "         [ 101, 1045, 2572,  ..., 3632, 2045,  102],\n",
              "         [ 101, 1045, 9471,  ..., 4687, 2040,  102],\n",
              "         ...,\n",
              "         [ 101, 1045, 2514,  ..., 2021, 2009,  102],\n",
              "         [ 101, 2651, 2003,  ..., 2021, 2057,  102],\n",
              "         [ 101, 1045, 2428,  ..., 1056, 2903,  102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')},\n",
              " {'cEXT': tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              "  'cNEU': tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1]),\n",
              "  'cAGR': tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0]),\n",
              "  'cCON': tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1]),\n",
              "  'cOPN': tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1])}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = APP(config).to(device)"
      ],
      "metadata": {
        "id": "slSfxsKY6qd0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "running_loss_essay = 0\n",
        "running_loss_mbti = 0"
      ],
      "metadata": {
        "id": "jfW9iNoL6c_R"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []"
      ],
      "metadata": {
        "id": "Z2u00E9Zq12m"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch, (X, y) in enumerate(train_dataloader):\n",
        "    X = X.to(device)\n",
        "    pred = model(X)\n",
        "    loss = model.get_ocean_loss(pred, y)\n",
        "    loss.backward()\n",
        "    losses.append(loss.item())\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    running_loss_essay += loss.item()\n",
        "\n",
        "    print(f\"Batch: {batch}, loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DcapsxekqsFG",
        "outputId": "665afea6-3c4f-4f38-d101-7d2e3fb0c430"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch: 0, loss: 3.4724419116973877\n",
            "Batch: 1, loss: 19.214431762695312\n",
            "Batch: 2, loss: 5.626371383666992\n",
            "Batch: 3, loss: 31.21635627746582\n",
            "Batch: 4, loss: 8.93565559387207\n",
            "Batch: 5, loss: 6.338554382324219\n",
            "Batch: 6, loss: 14.080392837524414\n",
            "Batch: 7, loss: 9.736677169799805\n",
            "Batch: 8, loss: 47.3046989440918\n",
            "Batch: 9, loss: 19.922677993774414\n",
            "Batch: 10, loss: 9.28736686706543\n",
            "Batch: 11, loss: 197.3241424560547\n",
            "Batch: 12, loss: 74.55260467529297\n",
            "Batch: 13, loss: 66.97242736816406\n",
            "Batch: 14, loss: 65.64590454101562\n",
            "Batch: 15, loss: 25.27166748046875\n",
            "Batch: 16, loss: 12.270748138427734\n",
            "Batch: 17, loss: 3.7610936164855957\n",
            "Batch: 18, loss: 3.4852700233459473\n",
            "Batch: 19, loss: 4.4334716796875\n",
            "Batch: 20, loss: 3.527494192123413\n",
            "Batch: 21, loss: 4.101905345916748\n",
            "Batch: 22, loss: 3.5455498695373535\n",
            "Batch: 23, loss: 3.586784839630127\n",
            "Batch: 24, loss: 3.502426862716675\n",
            "Batch: 25, loss: 3.4954628944396973\n",
            "Batch: 26, loss: 3.4088282585144043\n",
            "Batch: 27, loss: 3.4873318672180176\n",
            "Batch: 28, loss: 3.480961322784424\n",
            "Batch: 29, loss: 3.4624202251434326\n",
            "Batch: 30, loss: 3.471935749053955\n",
            "Batch: 31, loss: 3.4348702430725098\n",
            "Batch: 32, loss: 3.5225062370300293\n",
            "Batch: 33, loss: 3.501394033432007\n",
            "Batch: 34, loss: 3.444986343383789\n",
            "Batch: 35, loss: 3.5483646392822266\n",
            "Batch: 36, loss: 3.4474968910217285\n",
            "Batch: 37, loss: 3.534414529800415\n",
            "Batch: 38, loss: 3.4333152770996094\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-067b0840c336>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ocean_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-3f02ba3c3e67>\u001b[0m in \u001b[0;36mget_ocean_loss\u001b[0;34m(self, predictions, labels)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mOCEAN_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cOPN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cCON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cEXT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cAGR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cNEU'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mOCEAN_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mOCEAN_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "dmv03iTeB7aN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = BERTEncoder(config).to(device)"
      ],
      "metadata": {
        "id": "v3Qlgf19_7ff"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []"
      ],
      "metadata": {
        "id": "b3IrVJvCLJ2L"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2"
      ],
      "metadata": {
        "id": "-lW0vIwFLRSy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(epochs):\n",
        "    for index, row in essays_df.iterrows():\n",
        "        text = row['TEXT']\n",
        "        labels = row[cat_columns].to_dict()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "        preds = model(bert_model(tokens))\n",
        "        del tokens\n",
        "        loss = model.get_ocean_loss(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss_essay += loss.item()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if index % 200 == 0:    # print every 2000 mini-batches\n",
        "            print(f'essay index: {index + 1} loss: {running_loss_essay / 200:.3f}')\n",
        "            running_loss_essay = 0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "gCahzsxuQQ6l",
        "outputId": "8cf43eaf-4bf9-40cb-fbc0-03e5da58cff2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-1335c51698af>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ocean_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-8809f62de26e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m   1016\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 2.81 MiB is free. Process 1319036 has 14.74 GiB memory in use. Of the allocated memory 13.82 GiB is allocated by PyTorch, and 39.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1):\n",
        "    for index, row in mbti_df.iterrows():\n",
        "        text = row['posts']\n",
        "        labels = row['type']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "        preds = model(bert_model(tokens))\n",
        "        loss = model.get_mbti_loss(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss_mbti += loss.item()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if index % 200 == 0:    # print every 2000 mini-batches\n",
        "            print(f'mbti index: {index + 1} loss: {running_loss_mbti / 200:.3f}')\n",
        "            running_loss_mbti = 0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "fwzpGcmqQq8w",
        "outputId": "b6ce74db-d66d-4d6f-9794-fb69e874b05e"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mbti index: 1 loss: 0.014\n",
            "mbti index: 201 loss: 2.769\n",
            "mbti index: 401 loss: 2.761\n",
            "mbti index: 601 loss: 2.759\n",
            "mbti index: 801 loss: 2.762\n",
            "mbti index: 1001 loss: 2.763\n",
            "mbti index: 1201 loss: 2.759\n",
            "mbti index: 1401 loss: 2.760\n",
            "mbti index: 1601 loss: 2.759\n",
            "mbti index: 1801 loss: 2.773\n",
            "mbti index: 2001 loss: 2.772\n",
            "mbti index: 2201 loss: 2.764\n",
            "mbti index: 2401 loss: 2.765\n",
            "mbti index: 2601 loss: 2.766\n",
            "mbti index: 2801 loss: 2.764\n",
            "mbti index: 3001 loss: 2.761\n",
            "mbti index: 3201 loss: 2.767\n",
            "mbti index: 3401 loss: 2.760\n",
            "mbti index: 3601 loss: 2.761\n",
            "mbti index: 3801 loss: 2.771\n",
            "mbti index: 4001 loss: 2.762\n",
            "mbti index: 4201 loss: 2.767\n",
            "mbti index: 4401 loss: 2.764\n",
            "mbti index: 4601 loss: 2.762\n",
            "mbti index: 4801 loss: 2.769\n",
            "mbti index: 5001 loss: 2.760\n",
            "mbti index: 5201 loss: 2.763\n",
            "mbti index: 5401 loss: 2.757\n",
            "mbti index: 5601 loss: 2.766\n",
            "mbti index: 5801 loss: 2.771\n",
            "mbti index: 6001 loss: 2.763\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-116939b04bed>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2788\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2790\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2792\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             )\n\u001b[1;32m   2895\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2896\u001b[0;31m             return self.encode_plus(\n\u001b[0m\u001b[1;32m   2897\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m         )\n\u001b[1;32m   2968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   2970\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m             )\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m                 \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m         \u001b[0;31m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             for token in self.basic_tokenizer.tokenize(\n\u001b[0m\u001b[1;32m    246\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msplit_special_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             ):\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# union() returns a new set by concatenating the two sets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mnever_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnever_split\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;31m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0xFFFD\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_is_whitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_is_control\u001b[0;34m(char)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 50\n",
        "for i in range(epochs):\n",
        "    for index, row in essays_df.iterrows():\n",
        "        loss = torch.tensor()\n",
        "        text = row['TEXT']\n",
        "        labels = row[cat_columns].to_dict()\n",
        "\n",
        "        tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "        preds = model(bert_model(tokens))\n",
        "        loss = model.get_ocean_loss(preds, labels) + loss\n",
        "        del preds, labels\n",
        "        if (i+1) % BATCH_SIZE == 0:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss_essay += loss.item()\n",
        "            losses.append(loss.item())\n",
        "            loss = 0.0\n",
        "            optimizer.zero_grad()\n",
        "        if index % 1 == 0:    # print every 2000 mini-batches\n",
        "            print(f'essay index: {index + 1} loss: {running_loss_essay / 1:.3f}')\n",
        "            running_loss_essay = 0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 980
        },
        "id": "LCTFZCwd7CH1",
        "outputId": "b0b955b5-5327-44f2-c3e5-8623b6915feb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "essay index: 1 loss: 0.000\n",
            "essay index: 2 loss: 0.000\n",
            "essay index: 3 loss: 0.000\n",
            "essay index: 4 loss: 0.000\n",
            "essay index: 5 loss: 0.000\n",
            "essay index: 6 loss: 0.000\n",
            "essay index: 7 loss: 0.000\n",
            "essay index: 8 loss: 0.000\n",
            "essay index: 9 loss: 0.000\n",
            "essay index: 10 loss: 0.000\n",
            "essay index: 11 loss: 0.000\n",
            "essay index: 12 loss: 0.000\n",
            "essay index: 13 loss: 0.000\n",
            "essay index: 14 loss: 0.000\n",
            "essay index: 15 loss: 0.000\n",
            "essay index: 16 loss: 0.000\n",
            "essay index: 17 loss: 0.000\n",
            "essay index: 18 loss: 0.000\n",
            "essay index: 19 loss: 0.000\n",
            "essay index: 20 loss: 0.000\n",
            "essay index: 21 loss: 0.000\n",
            "essay index: 22 loss: 0.000\n",
            "essay index: 23 loss: 0.000\n",
            "essay index: 24 loss: 0.000\n",
            "essay index: 25 loss: 0.000\n",
            "essay index: 26 loss: 0.000\n",
            "essay index: 27 loss: 0.000\n",
            "essay index: 28 loss: 0.000\n",
            "essay index: 29 loss: 0.000\n",
            "essay index: 30 loss: 0.000\n",
            "essay index: 31 loss: 0.000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f0502bedfdea>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ocean_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-8809f62de26e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1020\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         )\n\u001b[0;32m-> 1022\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1023\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    610\u001b[0m                 )\n\u001b[1;32m    611\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    613\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 2.81 MiB is free. Process 1319036 has 14.74 GiB memory in use. Of the allocated memory 13.82 GiB is allocated by PyTorch, and 39.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(np.arange(len(losses)), losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "jJNSHigp7KKK",
        "outputId": "c0bd0208-8a34-43b0-a999-263abe704f7a"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x79eb90ba3100>]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sK22m8imNwcN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}